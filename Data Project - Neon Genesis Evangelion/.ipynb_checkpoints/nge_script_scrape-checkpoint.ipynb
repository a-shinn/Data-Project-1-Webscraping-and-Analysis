{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2905159",
   "metadata": {},
   "source": [
    "# **Data Project Pt. 1: Webscraping the script of Neon Genesis Evangelion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2331e69e",
   "metadata": {},
   "source": [
    "In this notebook, I'll be pulling scripts from each episode of Neon Genesis Evangelion using the BeautifulSoup and Requests packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dffbf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import requests\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936530e",
   "metadata": {},
   "source": [
    "To get the scripts I'll be using for this analysis, I'll be pulling from scripts available online at https://www.animanga.com/scripts/anime_scripts_english.html. The urls follow a pattern in naming conventions, so I can pull each episode script sequentially with a loop.\n",
    "\n",
    "The resulting dataset is created by parsing through each page for separate lines of dialogue using regular expressions. I created two separate dictionaries to track the series on an episodic level and as a whole.\n",
    "\n",
    "The first dictionary contains every characters' lines, line count, and word count for the entire series, while the second is a dictionary of dictionaries containing the same data, but with keys for each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28d6a50",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4201243887.py, line 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 57\u001b[1;36m\u001b[0m\n\u001b[1;33m    time.sleep(1)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "character_lines = {}\n",
    "lines_by_episode = {}\n",
    "\n",
    "for i in range(1,27):\n",
    "    current_ep = {}\n",
    "\n",
    "    webpage = requests.get(f'https://www.animanga.com/scripts/textesgb/eva{i}.html')\n",
    "    soup = bs.BeautifulSoup(webpage.text, 'lxml')\n",
    "    page_text = soup.get_text()\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "    regex = re.compile(r\"(?m).+:[^*#:]+\\n (?:(?![#*]).)*\") # regular expression to match each separate line of dialogue\n",
    "    result = re.findall(regex,page_text)\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "    # iterating through each line of text, creating a key and word count\n",
    "    # if character not already in character_lines, otherwise adding to the existing entry\n",
    "    for text in result:\n",
    "        character_name = re.findall('[^:#]*',text)[0] # regex to pull the name of the character speaking\n",
    "        character_name = character_name.strip('!\"#$%(*+, ''-./:;<=>?@[\\]^_`{|}~') # stripping leading + trailing punctuation\n",
    "        word_count_line = len(text.split())-1\n",
    "        text=text.split('------------')[0].split('----------------------')[0].split('-----------------------------------------------------------------------')[0].split('-------------------------------------------------------')[0].split('--------------------------------------------------------------------')[0].replace('\\n','').replace('\\r','')\n",
    "        if character_name in character_lines:\n",
    "            character_lines[character_name]['Lines'].append(text)\n",
    "            character_lines[character_name]['Line Count'] += 1\n",
    "            character_lines[character_name]['Word Count'] += word_count_line\n",
    "        else:\n",
    "            character_lines[character_name] = {}\n",
    "            character_lines[character_name]['Lines'] = []\n",
    "            character_lines[character_name]['Lines'].append(text)\n",
    "            character_lines[character_name]['Line Count'] = 1\n",
    "            character_lines[character_name]['Word Count'] = word_count_line\n",
    "\n",
    "    for text in result:\n",
    "        character_name = re.findall('[^:#]*',text)[0] # regex to pull the name of the character speaking\n",
    "        character_name = character_name.strip('!\"#$%(*+, ''-./:;<=>?@[\\]^_`{|}~') # stripping leading + trailing punctuation\n",
    "        word_count_line = len(text.split())-1\n",
    "        text=text.split('------------')[0].split('----------------------')[0].split('-----------------------------------------------------------------------')[0].split('-------------------------------------------------------')[0].split('--------------------------------------------------------------------')[0].replace('\\n','').replace('\\r','')\n",
    "        if character_name in current_ep:\n",
    "            current_ep[character_name]['Lines'].append(text)\n",
    "            current_ep[character_name]['Line Count'] += 1\n",
    "            current_ep[character_name]['Word Count'] += word_count_line\n",
    "        else:\n",
    "            current_ep[character_name] = {}\n",
    "            current_ep[character_name]['Lines'] = []\n",
    "            current_ep[character_name]['Lines'].append(text)\n",
    "            current_ep[character_name]['Line Count'] = 1\n",
    "            current_ep[character_name]['Word Count'] = word_count_line\n",
    "\n",
    "        # ------------------------------------------------------------------------\n",
    "\n",
    "    lines_by_episode[i] = current_ep\n",
    "\n",
    "        # there shouldn't be a risk of overloading the server, but added a short wait in between URL requests just in case\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ecaa9b",
   "metadata": {},
   "source": [
    "Looking through the compiled list of characters and lines, there are clearly a lot of typos, ranging from misspelled names, to translator notes being included, to whitespace or punctuation causing errors in dialogue attribution.\n",
    "\n",
    "Next steps in order:\n",
    "\n",
    "0. Done in the previous step: leading punctuation and whitespace was stripped so that names with errors (ex: 'Shinji' and '  Shinji  ') are counted for the same key in each dictionary.\n",
    "\n",
    "1. Sorting dictionaries and filtering out \"names\" which only appear due to being in the translator notes on each page\n",
    "\n",
    "2. Converting into dataframes\n",
    "\n",
    "3. Merging the misspelled names of characters into the correctly spelled dataframe column and dropping the typos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0895e5",
   "metadata": {},
   "source": [
    "## Sorting, filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff965ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# entire series: dictionary - {character name:[line count, word count]}\n",
    "sorted_lines = dict(sorted(character_lines.items())) \n",
    "\n",
    "# individual episodes: dictionary of dictionaries - {episode number: {character name:[line count, word count]}}\n",
    "sorted_eps = {}\n",
    "for i in range(1,27):\n",
    "    sorted_eps[i] = dict(sorted(lines_by_episode[i].items())) \n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "# # filtering out common translator notes not in the script\n",
    "filter = ['Neon','EVA','Email','E-mail','http','title','episode','Episode','EPISODE','Nadia','Movie','Preview','Trail','0','4']\n",
    "\n",
    "for i in range(1,27):\n",
    "    sorted_eps[i] = {k:v for k, v in sorted_eps[i].items() if not any(x in k for x in filter)}\n",
    "\n",
    "sorted_lines = {k:v for k, v in sorted_lines.items() if not any(x in k for x in filter)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce5e2f6",
   "metadata": {},
   "source": [
    "## Converting to dataframe, merging misspelled names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139b46a",
   "metadata": {},
   "source": [
    "To my knowledge, there isn't really a simple way to automate this part, so I had to look through the dataset manually to find typos and also looked through the script to determine where some ambiguous names should be attributed (ex: 'Ikari' is the last name of three different characters, but the lines were spoken by Ikari Gendo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to dataframes\n",
    "df = pd.DataFrame(data=sorted_lines).T\n",
    "df.columns = ['Lines','Linecount','Wordcount']\n",
    "\n",
    "df1 = pd.DataFrame(data=sorted_eps)\n",
    "df1 = df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f4c0d3",
   "metadata": {},
   "source": [
    "With pandas, we can very simply combine dataframes. for this analysis, I'm just going to compile typos and aliases for the top 10 characters and a couple other important ones from a historical character popularity poll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1105c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merges all names in the list into the first name of the list in the df dataframe\n",
    "def character_merge(list1):\n",
    "    result = df.loc[list1[0]].copy()\n",
    "    for name in list1[1:]:\n",
    "        result += df.loc[name].copy()\n",
    "    df.loc[list1[0]] = result\n",
    "        \n",
    "shinji_list = ['Shinji','Shiji',\"Shinji'\",\"Shinji '\",'Shinji&Asuka']\n",
    "asuka_list = ['Asuka','Little Asuka','Shinji&Asuka']\n",
    "misato_list = ['Misato','Misato (thinking)','Mistato','Phone(Misato)']\n",
    "ritsuko_list = ['Ritsuko','Ritusko','Ritsukko','Rituko']\n",
    "ryoji_list = ['Ryoji','Ryouji','Ryouji (voice from the telephone)']\n",
    "gendo_list = ['Gendo','Gendou','Gendow','Ikari']\n",
    "fuyutsuki_list = ['Fuyutsuki','Fuyutsuki (voice)','Fuyutsuki(mono)','Fuyuzuki','Kouzou','Kozo','Kozou']\n",
    "\n",
    "character_merge(shinji_list)\n",
    "character_merge(asuka_list)\n",
    "character_merge(misato_list)\n",
    "character_merge(ritsuko_list)\n",
    "character_merge(ryoji_list)\n",
    "character_merge(gendo_list)\n",
    "character_merge(fuyutsuki_list)\n",
    "\n",
    "# dropping the columns that were merged into the correct column\n",
    "df.drop(['Shiji',\"Shinji'\",\"Shinji '\",'Shinji&Asuka','Little Asuka','Shinji&Asuka','Misato (thinking)','Mistato',\n",
    "         'Phone(Misato)','Ritusko','Ritsukko','Rituko','Ryouji','Ryouji (voice from the telephone)','Gendou',\n",
    "         'Gendow','Fuyutsuki (voice)','Fuyutsuki(mono)','Fuyuzuki','Kouzou','Kozo','Kozou'], inplace=True)\n",
    "\n",
    "df1.drop(['Shiji',\"Shinji'\",\"Shinji '\",'Shinji&Asuka','Little Asuka','Shinji&Asuka','Misato (thinking)','Mistato',\n",
    "         'Phone(Misato)','Ritusko','Ritsukko','Rituko','Ryouji','Ryouji (voice from the telephone)','Gendou',\n",
    "         'Gendow','Fuyutsuki (voice)','Fuyutsuki(mono)','Fuyuzuki','Kouzou','Kozo','Kozou'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76394193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to dataframe and exporting to excel sheet and JSON data\n",
    "df.to_excel('src/NGE_entire_series_lines.xlsx')\n",
    "df1.to_excel('src/NGE_lines_by_episode.xlsx')\n",
    "\n",
    "df.to_json('src/NGE_entire_series.json',orient='columns')\n",
    "df1.to_json('src/NGE_by_episode.json',orient='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb798cb",
   "metadata": {},
   "source": [
    "The dataframes are now exported to Excel files and JSON files for further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
