{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2905159",
   "metadata": {},
   "source": [
    "# **Webscraping the script of Neon Genesis Evangelion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dffbf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import requests\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2331e69e",
   "metadata": {},
   "source": [
    "In this notebook, I'll be pulling scripts from each episode of Neon Genesis Evangelion using the BeautifulSoup and Requests packages and parsing through for each separate line of dialogue using regular expressions. I created two separate dictionaries to track the number of times each character speaks and their word count. The first dictionary covers character lines and word count for the entire series, and the second is a dictionary of dictionaries which contains the same data, but for a given episode and the characters in that episode.\n",
    "\n",
    "To get the scripts I'll be using for this analysis, I'll be pulling from scripts available online at https://www.animanga.com/scripts/anime_scripts_english.html. The urls follow a pattern in naming conventions, so I can pull each episode script sequentially with a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28d6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_lines = {}\n",
    "lines_by_episode = {}\n",
    "\n",
    "\n",
    "for i in range(1,27):\n",
    "    current_ep = {}\n",
    "    \n",
    "    webpage = requests.get(f'https://www.animanga.com/scripts/textesgb/eva{i}.html')\n",
    "    soup = bs.BeautifulSoup(webpage.text, 'lxml')\n",
    "    page_text = soup.get_text()\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "    regex = re.compile(r\"(?m).+:[^*#:]+\\n\") # regular expression to match each separate line of dialogue\n",
    "    result = re.findall(regex,page_text)\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "    # iterating through each line of text, creating a key and word count\n",
    "    # if character not already in character_lines, otherwise adding to the existing entry\n",
    "    for text in result:\n",
    "        character_name = re.findall('[^:#]*',text)[0] # regex to pull the name of the character speaking\n",
    "        character_name = character_name.strip('!\"#$%&(*+, ''-./:;<=>?@[\\]^_`{|}~') # stripping leading + trailing punctuation\n",
    "        word_count_line = len(text.split())-1\n",
    "        if character_name in character_lines:\n",
    "            character_lines[character_name][0] += 1\n",
    "            character_lines[character_name][1] += word_count_line\n",
    "        else:\n",
    "            character_lines[character_name] = [1,word_count_line]\n",
    "            \n",
    "    for text in result:\n",
    "        character_name = re.findall('[^:#]*',text)[0] # regex to pull the name of the character speaking\n",
    "        character_name = character_name.strip('!\"#$%&(*+, ''-./:;<=>?@[\\]^_`{|}~') # stripping leading + trailing punctuation\n",
    "        word_count_line = len(text.split())-1\n",
    "        if character_name in current_ep:\n",
    "            current_ep[character_name][0] += 1\n",
    "            current_ep[character_name][1] += word_count_line\n",
    "        else:\n",
    "            current_ep[character_name] = [1,word_count_line]\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "    lines_by_episode[i] = current_ep\n",
    "    \n",
    "    # there shouldn't be a risk of overloading the server, but added a short wait in between URL requests just in case\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ad2e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neon Genesis Evangelion (Japanese title',\n",
       " 'Nadia',\n",
       " 'Email',\n",
       " 'http',\n",
       " 'Radio',\n",
       " 'Misato',\n",
       " 'Phone',\n",
       " 'Shinji',\n",
       " '',\n",
       " 'Female Voice',\n",
       " 'Male Voice',\n",
       " 'Fuyutsuki',\n",
       " 'Gendo',\n",
       " 'EPISODE',\n",
       " 'Aircraft',\n",
       " 'Sub Commander A',\n",
       " 'Sub Commander B',\n",
       " 'One of Cmdr',\n",
       " 'Sub Cmdr A',\n",
       " 'Sub Cmdr B',\n",
       " 'Cmdr-in-Chief',\n",
       " 'Commanders',\n",
       " 'Misato (thinking)',\n",
       " 'Cmdr',\n",
       " 'Announce']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(character_lines.keys())[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e265aa69",
   "metadata": {},
   "source": [
    "Looking through the compiled list of characters and lines, there are clearly a lot of typos, ranging from misspelled names, to translator notes being included, to whitespace or punctuation causing errors in dialogue attribution.\n",
    "\n",
    "The order these issues will be tackled in:\n",
    "\n",
    "0. Done in the previous step: leading punctuation and whitespace was stripped so that names with errors (ex: 'Shinji' and '  Shinji  ') are counted for the same key in each dictionary.\n",
    "\n",
    "1. Splitting up lines where multiple characters are speaking at the same time and attributing the line to each character individually. There are occasionally other ways the translators denote multiple characters speaking but splitting with '&' was the most common. Looped through each episode and also separately for the entire series.\n",
    "\n",
    "2. Removing \"names\" which only appear due to being in the translator notes on each page.\n",
    "\n",
    "3. Spelling errors (in excel, not in this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f23ed5c",
   "metadata": {},
   "source": [
    "### **1. Splitting names and adding values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e97a0d49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before splitting and adding for Shinji: [915, 6854] - (format: [linecount, wordcount])\n",
      "After: [933, 6917]\n"
     ]
    }
   ],
   "source": [
    "# Before:\n",
    "print(f'Before splitting and adding for Shinji:', character_lines['Shinji'], '- (format: [linecount, wordcount])')\n",
    "\n",
    "# for each episode\n",
    "for i in range(1,27):\n",
    "    shared_lines = [[key,val] for key, val in lines_by_episode[i].items() if re.search('&', key)]\n",
    "    \n",
    "    for item in shared_lines:\n",
    "        item[0] = item[0].split('&')\n",
    "        word_count_line = item[1][1]\n",
    "        for names in item[0]:\n",
    "            if names.strip() in lines_by_episode[i]:\n",
    "                lines_by_episode[i][names.strip()][0] += 1\n",
    "                lines_by_episode[i][names.strip()][1] += word_count_line\n",
    "            else:\n",
    "                lines_by_episode[i][names.strip()] = [1,word_count_line]\n",
    "                \n",
    "# for entire series\n",
    "shared_lines = [[key,val] for key, val in character_lines.items() if re.search('&', key)]\n",
    "\n",
    "for item in shared_lines:\n",
    "    item[0] = item[0].split('&')\n",
    "    item_shared_lines = item[1][0]\n",
    "    item_shared_words = item[1][1]\n",
    "    for names in item[0]:\n",
    "        if names.strip() in character_lines:\n",
    "            character_lines[names.strip()][0] += item_shared_lines\n",
    "            character_lines[names.strip()][1] += item_shared_words\n",
    "        else:\n",
    "            character_lines[names.strip()] = [item_shared_lines,item_shared_words]\n",
    "            \n",
    "print(f'After:', character_lines['Shinji'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6efc20",
   "metadata": {},
   "source": [
    "Now each time a line was spoken by multiple characters (ex: \"Shinji & Asuka\"), each character receives credit for the spoken line in their original dictionary key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8507fb9",
   "metadata": {},
   "source": [
    "### **2. Sorting dictionaries and removing extraneous keys**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76394193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire series: dictionary - {character name:[line count, word count]}\n",
    "sorted_lines = dict(sorted(character_lines.items())) \n",
    "\n",
    "# individual episodes: dictionary of dictionaries - {episode number: {character name:[line count, word count]}}\n",
    "sorted_eps = {}\n",
    "for i in range(1,27):\n",
    "    sorted_eps[i] = dict(sorted(lines_by_episode[i].items())) \n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "# filtering out common translator notes not in the script\n",
    "filter = ['Neon','EVA','Email','E-mail','http','title','episode','Episode','EPISODE','Nadia','Movie','Preview','Trail','0']\n",
    "\n",
    "for i in range(1,27):\n",
    "    sorted_eps[i] = {k:v for k, v in sorted_eps[i].items() if not any(x in k for x in filter)}\n",
    "\n",
    "sorted_lines = {k:v for k, v in sorted_lines.items() if not any(x in k for x in filter)}\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "# converting to dataframe and exporting to excel sheet\n",
    "df = pd.DataFrame(data=sorted_lines).T\n",
    "df.columns = ['Linecount','Wordcount']\n",
    "df.to_excel('src/NGE_entire_series_lines.xlsx')\n",
    "\n",
    "df1 = pd.DataFrame(data=sorted_eps)\n",
    "df1 = df1\n",
    "df1.to_excel('src/NGE_lines_by_episode.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c706ede",
   "metadata": {},
   "source": [
    "The dictionaries are now sorted and converted into pandas DataFrames, and exported to Excel files for further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
